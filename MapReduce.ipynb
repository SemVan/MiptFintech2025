{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b27cd-039d-4bda-8479-757ae51374c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c70d926-cc29-4283-be93-e0159066c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл уже есть.\n",
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "filename = \"heavy_data.csv\"\n",
    "if not os.path.exists(filename):\n",
    "    print(\"Генерируем данные...\")\n",
    "    df_dummy = pd.DataFrame(np.random.randint(0, 100, size=(3000000, 5)), columns=list('ABCDE'))\n",
    "    df_dummy.to_csv(filename, index=False)\n",
    "    print(f\"Готово! Файл {filename} создан.\")\n",
    "else:\n",
    "    print(\"Файл уже есть.\")\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed5510d-20a7-4196-b700-176353e89562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pandas_memory_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pandas_memory_test.py\n",
    "from memory_profiler import profile\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Этот декоратор теперь сработает, потому что мы запустим это как внешний скрипт\n",
    "@profile\n",
    "def load_heavy_process():\n",
    "    print(\"--- Шаг 1: Старт функции ---\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    print(\"--- Шаг 2: Загрузка CSV (Смотри на скачок памяти ниже!) ---\")\n",
    "    # Читаем целиком\n",
    "    df = pd.read_csv(\"heavy_data.csv\")\n",
    "    \n",
    "    print(\"--- Шаг 3: Агрегация ---\")\n",
    "    # Какая-нибудь операция\n",
    "    res = df.groupby('A').sum()\n",
    "    \n",
    "    print(\"--- Шаг 4: Конец ---\")\n",
    "    return res\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_heavy_process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "417c678e-7b8a-41c2-9fc3-339cff208190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Шаг 1: Старт функции ---\n",
      "--- Шаг 2: Загрузка CSV (Смотри на скачок памяти ниже!) ---\n",
      "--- Шаг 3: Агрегация ---\n",
      "--- Шаг 4: Конец ---\n",
      "Filename: pandas_memory_test.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     6     74.5 MiB     74.5 MiB           1   @profile\n",
      "     7                                         def load_heavy_process():\n",
      "     8     74.5 MiB      0.0 MiB           1       print(\"--- Шаг 1: Старт функции ---\")\n",
      "     9     74.5 MiB      0.0 MiB           1       time.sleep(0.5)\n",
      "    10                                             \n",
      "    11     74.5 MiB      0.0 MiB           1       print(\"--- Шаг 2: Загрузка CSV (Смотри на скачок памяти ниже!) ---\")\n",
      "    12                                             # Читаем целиком\n",
      "    13    441.4 MiB    366.9 MiB           1       df = pd.read_csv(\"heavy_data.csv\")\n",
      "    14                                             \n",
      "    15    441.4 MiB      0.0 MiB           1       print(\"--- Шаг 3: Агрегация ---\")\n",
      "    16                                             # Какая-нибудь операция\n",
      "    17    442.0 MiB      0.6 MiB           1       res = df.groupby('A').sum()\n",
      "    18                                             \n",
      "    19    442.0 MiB      0.0 MiB           1       print(\"--- Шаг 4: Конец ---\")\n",
      "    20    442.0 MiB      0.0 MiB           1       return res\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m memory_profiler pandas_memory_test.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7807e28e-0088-4e8c-9a57-a203d25833c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pandas_chunk_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pandas_chunk_test.py\n",
    "from memory_profiler import profile\n",
    "import pandas as pd\n",
    "\n",
    "@profile\n",
    "def load_with_chunks():\n",
    "    print(\"--- Читаем по кусочкам ---\")\n",
    "    chunk_size = 500000 \n",
    "    total_sum = 0\n",
    "    \n",
    "    # Итератор вместо загрузки всего файла\n",
    "    for chunk in pd.read_csv(\"heavy_data.csv\", chunksize=chunk_size):\n",
    "        # Память растет только на размер чанка, потом очищается\n",
    "        total_sum += chunk['A'].sum()\n",
    "        \n",
    "    print(f\"Сумма: {total_sum}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_with_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2afda916-c774-4259-a353-5cc1bf5bad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Читаем по кусочкам ---\n",
      "Сумма: 148461287\n",
      "Filename: pandas_chunk_test.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     4     74.7 MiB     74.7 MiB           1   @profile\n",
      "     5                                         def load_with_chunks():\n",
      "     6     74.7 MiB      0.0 MiB           1       print(\"--- Читаем по кусочкам ---\")\n",
      "     7     74.7 MiB      0.0 MiB           1       chunk_size = 500000 \n",
      "     8     74.7 MiB      0.0 MiB           1       total_sum = 0\n",
      "     9                                             \n",
      "    10                                             # Итератор вместо загрузки всего файла\n",
      "    11    261.4 MiB    186.5 MiB           7       for chunk in pd.read_csv(\"heavy_data.csv\", chunksize=chunk_size):\n",
      "    12                                                 # Память растет только на размер чанка, потом очищается\n",
      "    13    261.4 MiB      0.2 MiB           6           total_sum += chunk['A'].sum()\n",
      "    14                                                 \n",
      "    15    261.4 MiB      0.0 MiB           1       print(f\"Сумма: {total_sum}\")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m memory_profiler pandas_chunk_test.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a860d5-f183-4d70-9706-9d1972f8bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def hadoop_simulation():\n",
    "    # ЭТАП 1: MAPPER (Читает диск -> Пишет на диск)\n",
    "    # Hadoop не может передать данные в памяти между этапами, он сохраняет их\n",
    "    print(\"Starting Mapper Job...\")\n",
    "    with open(\"big_data.csv\", \"r\") as infile, open(\"intermediate_data.csv\", \"w\") as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            if row['category'] == 'A': # Фильтрация\n",
    "                # Пишем на диск! (Самая медленная операция в мире компьютеров)\n",
    "                writer.writerow([row['category'], row['A']])\n",
    "\n",
    "    # ЭТАП 2: SHUFFLE & SORT (Сортировка на диске)\n",
    "    # (Опустим код, но представьте, что здесь происходит сортировка файлов)\n",
    "\n",
    "    # ЭТАП 3: REDUCER (Читает диск -> Пишет на диск)\n",
    "    print(\"Starting Reducer Job...\")\n",
    "    total_sum = 0\n",
    "    with open(\"intermediate_data.csv\", \"r\") as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "            # Снова читаем с диска!\n",
    "            total_sum += int(row[1])\n",
    "            \n",
    "    print(f\"Result: {total_sum}\")\n",
    "    \n",
    "    # Очистка мусора (Hadoop это делает сам, но ценой времени)\n",
    "    os.remove(\"intermediate_data.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hadoop_simulation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25e239fe-b858-441e-8af7-e6ffd10f9e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/ivan7chuk/Library/Python/3.9/lib/python/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     result_df\u001b[38;5;241m.\u001b[39mexplain()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mprocess_with_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mprocess_with_spark\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_with_spark\u001b[39m():\n\u001b[0;32m----> 5\u001b[0m     spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkHero\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[*]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 1. Lazy Loading (Мгновенно, память не ест)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Spark не читает файл сейчас. Он создает план.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/sql/session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/core/context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/core/context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    208\u001b[0m         master,\n\u001b[1;32m    209\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/core/context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pyspark/java_gateway.py:111\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    114\u001b[0m     )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    117\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def process_with_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkHero\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # 1. Lazy Loading (Мгновенно, память не ест)\n",
    "    # Spark не читает файл сейчас. Он создает план.\n",
    "    df = spark.read.csv(\"big_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # 2. Transformations (Цепочка в памяти)\n",
    "    # В отличие от Hadoop, здесь не создаются промежуточные файлы на диске.\n",
    "    # Spark объединит filter и groupBy в один проход (Pipelining).\n",
    "    result_df = df.filter(F.col(\"category\") == \"A\") \\\n",
    "                  .groupBy(\"category\") \\\n",
    "                  .agg(F.sum(\"A\").alias(\"total\"))\n",
    "\n",
    "    # 3. Action\n",
    "    # Только здесь начнется работа.\n",
    "    # Spark разобьет файл на партиции (как мы делали чанки в Pandas),\n",
    "    # Раздаст их ядрам (local[*]),\n",
    "    # Посчитает всё в RAM и вернет результат.\n",
    "    result_df.show()\n",
    "    \n",
    "    # Демонстрация плана выполнения (Доказательство ума Spark)\n",
    "    result_df.explain()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_with_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab5bd6-e44c-4689-bf84-2402b20da938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сложная цепочка\n",
    "df1 = df.withColumn(\"x\", F.col(\"A\") * 2)\n",
    "df2 = df1.filter(F.col(\"x\") > 10)\n",
    "df3 = df2.groupBy(\"category\").count()\n",
    "\n",
    "# В Hadoop это было бы 3 Job-ы (чтение-запись-чтение-запись...).\n",
    "# В Spark это ОДИН проход по данным.\n",
    "# Catalyst Optimizer видит: \"Ага, умножение и фильтр можно сделать одновременно, пока строка лежит в кэше процессора\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4037d-8533-4970-b5cc-5016d445b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField, IntegerType\n",
    "\n",
    "# 1. Генерируем сложные данные (Заказ -> Список товаров)\n",
    "data_nested = [\n",
    "    (1, \"Vasya\", [\"apple\", \"banana\", \"coffee\"]),\n",
    "    (2, \"Petya\", [\"pizza\"]),\n",
    "    (3, \"Oleg\", []) # Пустой список\n",
    "]\n",
    "\n",
    "df_nested = spark.createDataFrame(data_nested, [\"order_id\", \"user\", \"items\"])\n",
    "\n",
    "print(\"Исходные данные (массив внутри ячейки):\")\n",
    "df_nested.show(truncate=False)\n",
    "\n",
    "# 2. EXPLODE (Магия Spark)\n",
    "# Превращаем массив в строки. Одна строка заказа превратится в три строки товаров.\n",
    "# Это стандартная операция нормализации логов.\n",
    "df_exploded = df_nested.select(\n",
    "    \"order_id\", \n",
    "    \"user\", \n",
    "    F.explode(\"items\").alias(\"item\") # explode уничтожает строки с пустыми массивами!\n",
    ")\n",
    "\n",
    "# for row in rows:\n",
    "#     for elem in row['items']:\n",
    "#         new_row = row\n",
    "#         row['item'] = elem\n",
    "#         yield new_row\n",
    "# Хинт: если нужны и пустые массивы, используйте explode_outer\n",
    "\n",
    "print(\"После explode (Нормализованная таблица):\")\n",
    "df_exploded.show()\n",
    "\n",
    "# 3. Агрегация обратно (Collect List)\n",
    "# Допустим, мы пофильтровали товары и хотим свернуть всё обратно в список\n",
    "df_collapsed = df_exploded.groupBy(\"order_id\", \"user\") \\\n",
    "    .agg(F.collect_list(\"item\").alias(\"items_list\"))\n",
    "# for group in groups:\n",
    "#     order_id = None\n",
    "#     user = None\n",
    "#     items = []\n",
    "#     for row in rows:\n",
    "#         order_id = row['order_id']\n",
    "#         user = row['user']\n",
    "#         items.append(row['item'])\n",
    "#     new_row = {\n",
    "#         'order_id': order_id,\n",
    "#         'user': user,\n",
    "#         'items_list': items\n",
    "#     }\n",
    "#     yield new_row\n",
    "print(\"Собрали обратно:\")\n",
    "df_collapsed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6e9ea-37b3-45fb-8188-e6d24a71b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Данные: Продажи по дням и отделам\n",
    "data_sales = [\n",
    "    (\"Dep_A\", \"2023-01-01\", 100),\n",
    "    (\"Dep_A\", \"2023-01-02\", 150), # Рост\n",
    "    (\"Dep_A\", \"2023-01-03\", 120), # Падение\n",
    "    (\"Dep_B\", \"2023-01-01\", 500),\n",
    "    (\"Dep_B\", \"2023-01-02\", 500), # Стабильность\n",
    "    (\"Dep_B\", \"2023-01-03\", 600)\n",
    "]\n",
    "df_sales = spark.createDataFrame(data_sales, [\"dept\", \"date\", \"sales\"])\n",
    "\n",
    "# ОПРЕДЕЛЯЕМ ОКНО\n",
    "# \"Для каждого отдела, отсортировав по дате...\"\n",
    "window_spec = Window.partitionBy(\"dept\").orderBy(\"date\")\n",
    "\n",
    "df_analytics = df_sales.withColumn(\n",
    "    \"prev_day_sales\", \n",
    "    F.lag(\"sales\", 1).over(window_spec) # LAG - взять значение из предыдущей строки\n",
    ").withColumn(\n",
    "    \"diff\", \n",
    "    F.col(\"sales\") - F.col(\"prev_day_sales\") # Разница\n",
    ").withColumn(\n",
    "    \"running_total\",\n",
    "    F.sum(\"sales\").over(window_spec) # Нарастающий итог (Running Total)\n",
    ")\n",
    "\n",
    "print(\"Аналитика изменений (Lag и Running Total):\")\n",
    "df_analytics.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fa884f6-d630-413b-8c2c-2c7cdf93e14e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Грязные данные: Цена как строка, даты кривые\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data_dirty \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTV\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50000.00\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m01/01/2023\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;66;03m# Другой формат даты\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaptop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100k\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-Hello\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;66;03m# Мусор\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1500\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-01-10\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m ]\n\u001b[0;32m----> 8\u001b[0m df_dirty \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mcreateDataFrame(data_dirty, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduct\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_str\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_str\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Очистка\u001b[39;00m\n\u001b[1;32m     11\u001b[0m df_clean \u001b[38;5;241m=\u001b[39m df_dirty\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     13\u001b[0m     F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdouble\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Безопасный каст (мусор станет null)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     F\u001b[38;5;241m.\u001b[39mexpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice IS NOT NULL AND date_unified IS NOT NULL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Грязные данные: Цена как строка, даты кривые\n",
    "data_dirty = [\n",
    "    (\"TV\", \"50000.00\", \"2023-01-01\"),\n",
    "    (\"Phone\", \"null\", \"01/01/2023\"), # Другой формат даты\n",
    "    (\"Laptop\", \"100k\", \"2023-Hello\"), # Мусор\n",
    "    (\"Mouse\", \"1500\", \"2023-01-10\"),\n",
    "]\n",
    "df_dirty = spark.createDataFrame(data_dirty, [\"product\", \"price_str\", \"date_str\"])\n",
    "\n",
    "# Очистка\n",
    "df_clean = df_dirty.withColumn(\n",
    "    \"price\", \n",
    "    F.col(\"price_str\").cast(\"double\") # Безопасный каст (мусор станет null)\n",
    ").withColumn(\n",
    "    \"date_unified\",\n",
    "    # COALESCE - верни первое не null значение\n",
    "    F.coalesce(\n",
    "        F.to_date(\"date_str\", \"yyyy-MM-dd\"), # Пробуем формат 1\n",
    "        F.to_date(\"date_str\", \"dd/MM/yyyy\")  # Пробуем формат 2\n",
    "    )\n",
    ").withColumn(\n",
    "    \"is_valid\",\n",
    "    # SQL выражение прямо в коде! Очень удобно для сложной логики.\n",
    "    F.expr(\"price IS NOT NULL AND date_unified IS NOT NULL\")\n",
    ")\n",
    "\n",
    "print(\"Очистка и валидация:\")\n",
    "df_clean.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b0fd1a-f69f-4144-8980-f8132d846a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПЛОХОЙ ПРИМЕР: Итерация через Python (Looping)\n",
    "def anti_pattern_demo():\n",
    "    # .collect() забирает ВСЕ данные на драйвер (ваш ноутбук)\n",
    "    rows = df_sales.collect() \n",
    "    \n",
    "    # Loop происходит на ОДНОМ ядре (на драйвере)\n",
    "    # Вся мощь кластера простаивает!\n",
    "    results = []\n",
    "    for row in rows:\n",
    "        # Медленная питоновская логика\n",
    "        if row['sales'] > 100:\n",
    "            results.append(row['dept'] + \"_high\")\n",
    "        else:\n",
    "            results.append(row['dept'] + \"_low\")\n",
    "            \n",
    "    print(results[:5])\n",
    "\n",
    "# ХОРОШИЙ ПРИМЕР: Векторная операция\n",
    "def good_pattern_demo():\n",
    "    # Работает на кластере, параллельно, без загрузки на драйвер\n",
    "    df_sales.withColumn(\n",
    "        \"tag\", \n",
    "        F.when(F.col(\"sales\") > 100, F.concat(F.col(\"dept\"), F.lit(\"_high\")))\n",
    "         .otherwise(F.concat(F.col(\"dept\"), F.lit(\"_low\")))\n",
    "    ).show()\n",
    "\n",
    "good_pattern_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a459e6-8572-4707-a62d-6461255ec4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные: Продажи по магазинам и месяцам\n",
    "data_pivot = [\n",
    "    (\"Shop_1\", \"Jan\", 100),\n",
    "    (\"Shop_1\", \"Feb\", 150),\n",
    "    (\"Shop_1\", \"Mar\", 120),\n",
    "    (\"Shop_2\", \"Jan\", 200),\n",
    "    (\"Shop_2\", \"Feb\", 210),\n",
    "]\n",
    "df_pivot = spark.createDataFrame(data_pivot, [\"shop\", \"month\", \"amount\"])\n",
    "\n",
    "print(\"Исходная (Long format):\")\n",
    "df_pivot.show()\n",
    "\n",
    "# PIVOT (Строки -> Колонки)\n",
    "# Для каждого магазина сделать колонки Jan, Feb, Mar с суммой продаж\n",
    "df_pivoted = df_pivot.groupBy(\"shop\") \\\n",
    "    .pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]) \\\n",
    "    .sum(\"amount\") \\\n",
    "    .na.fill(0) # Заменяем null на 0, если данных не было\n",
    "\n",
    "print(\"Повернутая (Wide format):\")\n",
    "df_pivoted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141968fa-dd20-4012-a911-a6a44d249c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = spark.createDataFrame([(\"2023-01-31\",), (\"2023-02-28\",)], [\"date_str\"])\n",
    "\n",
    "df_dates_calc = df_dates.select(\n",
    "    F.col(\"date_str\"),\n",
    "    # 1. Сложение дат\n",
    "    F.date_add(F.col(\"date_str\"), 10).alias(\"plus_10_days\"),\n",
    "    F.add_months(F.col(\"date_str\"), 1).alias(\"plus_1_month\"), # Умное сложение (учитывает длину месяца)\n",
    "    \n",
    "    # 2. Текущая дата и разница\n",
    "    F.current_date().alias(\"today\"),\n",
    "    F.datediff(F.current_date(), F.col(\"date_str\")).alias(\"days_passed\"),\n",
    "    \n",
    "    # 3. Trunc (Округление до начала месяца/года)\n",
    "    F.trunc(F.col(\"date_str\"), \"month\").alias(\"start_of_month\"),\n",
    "    \n",
    "    # 4. Форматирование\n",
    "    F.date_format(F.col(\"date_str\"), \"dd.MM.yyyy -- EEEE\").alias(\"pretty_date\")\n",
    ")\n",
    "\n",
    "df_dates_calc.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa854b7c-926f-43e6-b69a-feb1dfcc63c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим \"плохой\" CSV файл программно\n",
    "bad_csv_content = \"\"\"id,name,age\n",
    "1,Vasya,25\n",
    "2,Petya,Corrupted_Age\n",
    "3,Oleg,30\n",
    "4,Broken_Row\n",
    "\"\"\"\n",
    "with open(\"bad_data.csv\", \"w\") as f:\n",
    "    f.write(bad_csv_content)\n",
    "\n",
    "print(\"Читаем плохой файл...\")\n",
    "\n",
    "# Режим PERMISSIVE (по умолчанию) с колонкой _corrupt_record\n",
    "df_bad = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_raw_error\") \\\n",
    "    .csv(\"bad_data.csv\")\n",
    "\n",
    "# Что произошло:\n",
    "# 1. Хорошие строки распарсились.\n",
    "# 2. Плохие строки попали в _raw_error.\n",
    "# 3. Остальные поля в плохих строках стали null.\n",
    "df_bad.show()\n",
    "\n",
    "# Как это чистить в проде:\n",
    "clean_df = df_bad.filter(F.col(\"_raw_error\").isNull()).drop(\"_raw_error\")\n",
    "error_df = df_bad.filter(F.col(\"_raw_error\").isNotNull())\n",
    "\n",
    "print(\"Чистые данные:\")\n",
    "clean_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021ab87-302a-4a6e-b9e2-871c76d14a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем данные с повторами (1 млн строк)\n",
    "big_range = spark.range(0, 1000000).withColumn(\"group_id\", F.lit(1))\n",
    "# Дублируем, чтобы проверить distinct\n",
    "big_data = big_range.union(big_range) \n",
    "\n",
    "start = time.time()\n",
    "exact = big_data.select(F.countDistinct(\"id\")).collect()[0][0]\n",
    "print(f\"Точный подсчет: {exact}, Время: {time.time() - start:.2f} сек\")\n",
    "\n",
    "start = time.time()\n",
    "# rsd - максимальная относительная ошибка. 0.05 = 5%\n",
    "approx = big_data.select(F.approx_count_distinct(\"id\", rsd=0.05)).collect()[0][0]\n",
    "print(f\"Быстрый подсчет: {approx}, Время: {time.time() - start:.2f} сек\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110fc6c3-81ed-4d28-81de-41f973106aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users = [(1, \"user\", 50), (2, \"admin\", 0), (3, \"user\", 1000), (4, \"guest\", 0)]\n",
    "df_users = spark.createDataFrame(data_users, [\"id\", \"role\", \"spent\"])\n",
    "\n",
    "# Сложная логика тегирования\n",
    "df_tagged = df_users.withColumn(\"segment\", \n",
    "    F.when(F.col(\"role\") == \"admin\", \"Internal\")\n",
    "     .when((F.col(\"role\") == \"user\") & (F.col(\"spent\") > 500), \"Premium User\")\n",
    "     .when(F.col(\"spent\") > 0, \"Active User\")\n",
    "     .otherwise(\"Ghost\")\n",
    ")\n",
    "\n",
    "df_tagged.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
